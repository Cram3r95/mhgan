# SoPhie configuration file

# Model hyperparameters

use_gpu: 1
dataset_name: argoverse_motion_forecasting_dataset
dataset:
    path: data/datasets/argoverse/motion-forecasting/
    split: "train"
    batch_size: 128 # 32 max
    split_percentage: 0.002
    shuffle: False # False to check the input data, always set to True
    class_balance: 0.7 # % of straight trajectories (considering the AGENT). Remaining % are curved trajectories
                       # (again, considering the AGENT). -1.0 if no class balance is used (get_item takes the corresponding
                       # sequence regardless if it is straight or curved)
    num_workers: 0
optim_parameters:
    g_learning_rate: 1.0e-4
    g_weight_decay: 0
    d_learning_rate: 1.0e-4
    d_weight_decay: 0
hyperparameters:
    output_single_agent: True
    classic_trainer: False
    tensorboard_active: True
    num_iterations: 20000 #1608 #20000
    num_epochs: 100 #75
    d_steps: 2
    g_steps: 1
    timing: 0 # Waits for all kernels in all streams on a CUDA device to complete.
    print_every: 2
    checkpoint_every: 100
    output_dir: "save/argoverse/test/"
    exp_description: "Class balance. Local frame coordinates instead of Map frame. Training with Multiple Agents. Temporal decoder (only the important AGENT). No coordinates normalization"
    checkpoint_name: "0"
    checkpoint_start_from: 
    restore_from_checkpoint: 
    clipping_threshold_d: 0
    clipping_threshold_g: 1.5
    best_k: 10
    l2_loss_weight: 1 # If different from 0, L2 loss is considered when training
    num_samples_check: 5000
    obs_origin: 20 # This frame will be the origin, tipically the first observation (1) or last observation 
                   # (obs_len) of the AGENT (object to be predicted in Argoverse 1.0). Note that in the code
                   # it will be 0 and 19 respectively
    obs_len: &obs_len 20 
    pred_len: &pred_len 30 # Must be 0 for the split test since we do not have the predictions of the agents
                           # Only the observations (0 to obs_len-1 past observations)
    num_agents_per_obs: &num_agents_per_obs 10
    distance_threshold: 40 # It depends on a statistical study (see get_sequences_as_array function), 
                           # where we determine the mean distance of the AGENT w.r.t the ego-vehicle
                           # in the obs_len-th frame
    hidden_dim_lstm_decoder: &hidden_dim_lstm_decoder 128

# SoPhie model

sophie:
    generator: 
        # Visual Extractor (CNN)
        visual_extractor:
            type: "vgg19"
            vgg:
                vgg_type: 19
                batch_norm: False
                pretrained: True
                features: True

        # Joint Extractor
        joint_extractor:
            type: "encoder_sort"
            config:
                encoder:
                    num_layers: 1 
                    hidden_dim: *num_agents_per_obs
                    emb_dim: 16 # embedding input from mlp
                    mlp_config:
                        dim_list: [2, 16] # From 2 (x,y) to 16 (original embedding of the paper)
                        activation: 'relu'
                        batch_norm: False
                        dropout: 0
                    dropout: 0
        physical_attention:
            linear_decoder:
                in_features: *hidden_dim_lstm_decoder # Original paper
                out_features: 512 # Original paper
            linear_feature:
                in_features: 324 # Original paper. From 600 x 600 images to 18 x 18 ( = 324) activation maps
                out_features: 2
            softmax:
                dim: 0
        social_attention:
            linear_decoder:
                in_features: *hidden_dim_lstm_decoder # Original paper
                out_features: # Past Observations x Number of agents. Fill in the code
            linear_feature:
                in_features: *num_agents_per_obs 
                out_features: 2
            softmax:
                dim: 0
        decoder:
            linear_1:
                input_dim: *num_agents_per_obs
                output_dim: 64 # Original paper

            # LSTM

            num_layers: 1
            hidden_dim: *hidden_dim_lstm_decoder
            emb_dim: 64
            dropout: 0
            pred_len: *pred_len

            linear_2:
                input_dim: *hidden_dim_lstm_decoder
                output_dim: 64 # Original paper
            mlp_config:
                dim_list: [64, 128, 64, 2] 
                activation: ''
                batch_norm: True
                dropout: 0.5
            linear_3: # Not used at this moment (agentscorrector in decoder module)
                input_dim:  
                output_dim: 

        noise:
            noise_type: "gauss" # gauss or uniform

    discriminator:
        # Encoder
        encoder:
            num_layers: 1
            hidden_dim: 64
            emb_dim: 16 # embedding input from mlp
            mlp_config:
                dim_list: [2, 16] # From 2 (x,y) to 16 (original embedding of the paper)
                activation: 'relu'
                batch_norm: True
                dropout: 0.5
            dropout: 0

        # Classifier
        classifier:
            mlp_config:
                dim_list: [64, 1024, 1]
                activation: 'relu'
                batch_norm: True
                dropout: 0.5